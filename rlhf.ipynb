{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning (RLHF)\n",
    "The final stages of training involve further fine-tuning the model via reinforcement learning. This reinforcement learning utilises the previously trained reward model as a measure of an output's reward.\n",
    "\n",
    "Several distinct features are at play here:\n",
    "1. The prompt/punchline combinations are randomised, to emulate the original format of RLHF.\n",
    "2. Performance of the trained model is tracked live, via WandB."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configures the training functions\n",
    "tqdm.pandas()\n",
    "config = PPOConfig(\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    learning_rate=(1.47e-5) * 2,\n",
    "    log_with=\"wandb\",\n",
    "    batch_size=8,\n",
    "    forward_batch_size=1,\n",
    ")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "set_seed(config.seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sets up model, tokenizer and optimizer.\n",
    "# We set the pad token to the eos token for GPT models only.\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "ref_model = create_reference_model(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Models/Tokenizer', local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loads the data into a dataset.\n",
    "dataset = load_dataset(\"csv\", data_files=\"trimmed_cah_data.csv\", split=\"train\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize dataset for the training. Only the prompt row is utilised.\n",
    "def tokenize(sample):\n",
    "    info = ast.literal_eval(sample['info'])\n",
    "    sample['input_ids'] = tokenizer.encode(info['prompt'], truncation=True)\n",
    "    sample['query'] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Apply to dataset, split into 80/20 split.\n",
    "dataset = dataset.map(tokenize, batched=False)\n",
    "dataset.set_format(type=\"torch\")\n",
    "dataset = dataset.train_test_split(shuffle=True, test_size=0.2)['train']\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up trainer and reward model, via the local model.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config, model, ref_model=ref_model, tokenizer=tokenizer, dataset=dataset, data_collator=collator\n",
    ")\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained('Models/Model', num_labels=1, local_files_only=True).to(\n",
    "    ppo_trainer.accelerator.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generation args.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "output_min_length = 4\n",
    "output_max_length = 30\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_save_path = \"/Models/CAH-Model\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training process\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Generates a response from the model.\n",
    "    # An input queries list is generated to store the prompts for every generated punchline.\n",
    "    response_tensors = []\n",
    "    input_queries = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "\n",
    "        # The punchlines are decoded/detokenized into text.\n",
    "        input_queries.append(TreebankWordDetokenizer().detokenize([tokenizer.decode(x) for x in query]))\n",
    "\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    # A reward is computed from the model by fitting the same format the reward model trainer utilises.\n",
    "    texts = batch[\"response\"]\n",
    "    new_texts = []\n",
    "    for index, response in enumerate(texts):\n",
    "        new_texts.append(response + \" \" + tokenizer.bos_token + \" \" + input_queries[index])\n",
    "\n",
    "    reward_inputs = tokenizer(new_texts, padding=True, truncation=True, return_tensors=\"pt\").to(\n",
    "        ppo_trainer.accelerator.device\n",
    "    )\n",
    "    logits = reward_model(**reward_inputs).logits.float()\n",
    "    reward_labels = (logits[:, 0]).tolist()\n",
    "\n",
    "    rewards = [torch.tensor(output) for output in reward_labels]\n",
    "\n",
    "    # A PPO step is completed.\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # Save model every 100 epochs (?)\n",
    "    if epoch % 100 == 0:\n",
    "        if ppo_trainer.accelerator.is_main_process:\n",
    "            print(\"Saving after 100 epochs.\")\n",
    "            ppo_trainer.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
