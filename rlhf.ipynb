{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "The final stages of training involve further fine-tuning the model via proximal policy optimisation. This reinforcement learning utilises a previously trained reward model as a measure of an output's reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-07T05:59:42.360281Z",
     "iopub.status.busy": "2023-09-07T05:59:42.359916Z",
     "iopub.status.idle": "2023-09-07T05:59:47.409387Z",
     "shell.execute_reply": "2023-09-07T05:59:47.408718Z",
     "shell.execute_reply.started": "2023-09-07T05:59:42.360258Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports libraries necessary for the reinforcement learning process.\n",
    "import torch, gc\n",
    "from datasets import Dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "# from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from RLHF.trainer import RLTrainer\n",
    "from RLHF.utils import create_reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-07T05:59:47.411467Z",
     "iopub.status.busy": "2023-09-07T05:59:47.410941Z",
     "iopub.status.idle": "2023-09-07T05:59:47.446567Z",
     "shell.execute_reply": "2023-09-07T05:59:47.445987Z",
     "shell.execute_reply.started": "2023-09-07T05:59:47.411435Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configures basic functions.\n",
    "tqdm.pandas()\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "# Defines hyperparameters.\n",
    "# TODO: redo this\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-07T05:59:47.448292Z",
     "iopub.status.busy": "2023-09-07T05:59:47.448033Z",
     "iopub.status.idle": "2023-09-07T05:59:50.943261Z",
     "shell.execute_reply": "2023-09-07T05:59:50.942497Z",
     "shell.execute_reply.started": "2023-09-07T05:59:47.448271Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sets up the model, reference model and tokenizer.\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Iterations/BERT-final', local_files_only=True)\n",
    "ref_model = create_reference_model(model)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sets up the optimizer and learning rate scheduler.\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-07T05:59:50.945200Z",
     "iopub.status.busy": "2023-09-07T05:59:50.944950Z",
     "iopub.status.idle": "2023-09-07T05:59:57.875923Z",
     "shell.execute_reply": "2023-09-07T05:59:57.875308Z",
     "shell.execute_reply.started": "2023-09-07T05:59:50.945179Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loads the data into a dataset and then groups the game rounds into rows.\n",
    "dataset = pd.read_csv('Data/random_cah_data.csv')\n",
    "grouped_data = dataset.groupby('round_id').apply(lambda x: pd.Series({'jokes': x['white_card_text'].tolist(),\n",
    "                                                                 'clean_jokes': x['clean_white_card_text'].tolist(),\n",
    "                                                                 'black_card_text': x['black_card_text'].iat[0],\n",
    "                                                                 'won': x['won'].tolist()})).reset_index()\n",
    "\n",
    "# Converts the Pandas data to a Dataset, before splitting and shuffling it.\n",
    "dataset = Dataset.from_pandas(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-07T05:59:57.877163Z",
     "iopub.status.busy": "2023-09-07T05:59:57.876856Z",
     "iopub.status.idle": "2023-09-07T06:01:44.869487Z",
     "shell.execute_reply": "2023-09-07T06:01:44.868819Z",
     "shell.execute_reply.started": "2023-09-07T05:59:57.877142Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizes tbe dataset for the training.\n",
    "def tokenize(sample):\n",
    "    # Initialises lists to allow for a combination of prompts/punchlines.\n",
    "    prompt = sample['black_card_text']\n",
    "    punchlines = sample['jokes']\n",
    "    clean_punchlines = sample['clean_jokes']\n",
    "    combined = []\n",
    "\n",
    "    # For each punchline in the current sample, combine the punchline with the prompt to make a combined joke.\n",
    "    for index, punchline in enumerate(punchlines):\n",
    "        if prompt.count('_____') == 0:\n",
    "            combined.append(f\"{prompt} {punchline}\")\n",
    "        else:\n",
    "            combined.append(prompt.replace(\"_____\", clean_punchlines[index]))\n",
    "\n",
    "    # Tokenizes the jokes formed through these combinations.\n",
    "    tokenized_examples = tokenizer(combined, max_length=335, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # We also store the winning joke's label, the round ID and the plaintext combined jokes.\n",
    "    tokenized_examples['label'] = sample['won']\n",
    "    tokenized_examples['round_id'] = sample['round_id']\n",
    "    tokenized_examples['combined'] = combined\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# Apply to dataset, split into 80/20 split.\n",
    "original_columns = dataset.column_names\n",
    "dataset = dataset.map(tokenize, batched=False, remove_columns=original_columns)\n",
    "dataset.set_format(type=\"torch\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-07T06:01:44.870567Z",
     "iopub.status.busy": "2023-09-07T06:01:44.870354Z",
     "iopub.status.idle": "2023-09-07T06:01:51.645762Z",
     "shell.execute_reply": "2023-09-07T06:01:51.645054Z",
     "shell.execute_reply.started": "2023-09-07T06:01:44.870549Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the reinforcement learning trainer.\n",
    "trainer = RLTrainer(\n",
    "    \"RL Bert\", model, ref_model=ref_model, tokenizer=tokenizer, optimizer=optimizer, dataset=dataset, data_collator=collator\n",
    ")\n",
    "\n",
    "# Set up the reward model, if required.\n",
    "toxicity_tokenizer = RobertaTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "toxicity_model = RobertaForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "# reward_model = AutoModelForSequenceClassification.from_pretrained('Models/Model', num_labels=1, local_files_only=True).to(trainer.accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-07T06:01:51.647475Z",
     "iopub.status.busy": "2023-09-07T06:01:51.647214Z",
     "iopub.status.idle": "2023-09-07T17:29:33.832396Z",
     "shell.execute_reply": "2023-09-07T17:29:33.830998Z",
     "shell.execute_reply.started": "2023-09-07T06:01:51.647447Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Trains the model using RL.\n",
    "for epoch, batch in tqdm(enumerate(trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    mask_tensors = batch['attention_mask']\n",
    "    label_tensors = batch[\"label\"]\n",
    "\n",
    "    # Generates a response from the model.\n",
    "    # We also store the jokes in input_queries.\n",
    "    reward_tensors = []\n",
    "    response_tensors = []\n",
    "    response_list = []\n",
    "\n",
    "    # Clears up memory.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Iterates through every query in the batch.\n",
    "    for q_index, query in enumerate(query_tensors):\n",
    "        response_tensors.append([])\n",
    "        response_list.append([])\n",
    "        # A response for each joke is generated.\n",
    "        for j_index, joke in enumerate(query):\n",
    "            with torch.no_grad():\n",
    "                response = trainer.generate(joke, mask_tensors[q_index][j_index]).logits\n",
    "\n",
    "            # A separate list and tensor is generated.\n",
    "            response_tensors[q_index].append(response[0][0])\n",
    "            response_list[q_index].append(response.tolist())\n",
    "\n",
    "        # Computes the reward for each joke response.\n",
    "        # The index of the joke with the maximum score is grabbed.\n",
    "        max_ind = list(np.flatnonzero(response_list[q_index] == np.max(response_list[q_index])))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        # The index of the joke that is chosen joke, by the player, is grabbed.\n",
    "        truth_ind = list(np.flatnonzero(batch['label'][q_index].tolist() == np.max(batch['label'][q_index].tolist())))[0]\n",
    "\n",
    "        # Calculates the reward for each joke/prediction.\n",
    "        with torch.no_grad():\n",
    "            toxicity_labels = toxicity_model(**toxicity_tokenizer(batch['combined'][q_index], padding=True, truncation=True, return_tensors=\"pt\")).logits.float()\n",
    "            toxicity_labels = (toxicity_labels[:, 0] * -1).tolist()\n",
    "\n",
    "        reward_tensors.append([])\n",
    "        for index, joke in enumerate(response_list[q_index]):\n",
    "            curr_reward = toxicity_labels[index]\n",
    "            reward_tensors[q_index].append(torch.tensor(curr_reward).to(\"cuda:0\"))\n",
    "        print(\"Reward: \", float(reward_tensors[q_index][max_ind]))\n",
    "\n",
    "    # Stores the rewards.\n",
    "    rewards = [torch.stack(output) for output in reward_tensors]\n",
    "\n",
    "    # A PPO step is completed.\n",
    "    stats = trainer.step(query_tensors, mask_tensors, rewards)\n",
    "    # trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # Save model every 10 epochs.\n",
    "    if epoch % 5 == 0:\n",
    "        if trainer.accelerator.is_main_process:\n",
    "            print(f\"Saving after {epoch} epochs.\")\n",
    "            trainer.save_pretrained('Iterations/BERT-final/Toxicity/RLHF')\n",
    "\n",
    "            if epoch % 25 == 0 and not epoch == 0:\n",
    "                print(f\"Saving extra copy after {epoch} epochs.\")\n",
    "                trainer.save_pretrained(f'Iterations/BERT-final/Toxicity/RLHF{epoch}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
