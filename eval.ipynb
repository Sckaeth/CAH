{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The code within this notebook is primarily for evaluating the model's performance in an isolated setting.\n",
    "\n",
    "Thus, it will support:\n",
    "1. Testing the model's performance against the test dataset.\n",
    "2. Testing the performance of other models/methodologies on the test dataset.\n",
    "3. Evaluating the model on random samples of the test dataset.\n",
    "4. Testing the performance of the model on individual, randomly generated, game rounds.\n",
    "\n",
    "The following metrics will be printed for each evaluation:\n",
    "- The success rate of predictions.\n",
    "- The most misclassified prompts & punchlines.\n",
    "- The most accurately classified prompts & punchlines.\n",
    "- The average reward of predictions, if enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libraries relevant to the evaluation process.\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch, gc, random, re, csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the trained model off an input dataset.\n",
    "def eval_model(dataset, model_name='Iterations/BERT-fixed', reward=False):\n",
    "    # Defines necessary evaluation variables.\n",
    "    if model_name != \"popularity\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        tokenizer_kwargs = {'padding':True, 'truncation':True, 'max_length':512}\n",
    "        generator = pipeline(\"text-classification\", model=model_name, batch_size = 10, tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "\n",
    "    if reward:\n",
    "        toxicity_tokenizer = RobertaTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "        toxicity_model = RobertaForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "\n",
    "    batch_len = len(dataset)\n",
    "\n",
    "    # Defines the evaluation metrics.\n",
    "    t1_accuracy, t2_accuracy, t3_accuracy, rewards, truth_rewards = 0, 0, 0, 0, 0\n",
    "    classified = {\"prompts\": [], \"punchlines\":[]}\n",
    "    misclassified = {\"prompts\": [], \"punchlines\": []}\n",
    "    incorrect = {\"punchlines\": []}\n",
    "\n",
    "    # Iterates through each batch in the data, with each batch representing a round played.\n",
    "    for batch in tqdm(dataset, desc=\"Evaluation progress\"):\n",
    "        # Generates a score for each joke in the batch.\n",
    "\n",
    "        if model_name == \"popularity\":\n",
    "            # scores = batch['win_rate']\n",
    "            scores = [0] * 10\n",
    "            user_choice = int(batch['user_choice'][0])\n",
    "            scores[user_choice] = 1\n",
    "        else:\n",
    "            scores = []\n",
    "            for out in generator(KeyDataset(batch, \"inputs\"), **tokenizer_kwargs):\n",
    "                scores.append(out['score'])\n",
    "\n",
    "        # Selects the index of the maximum score. If there is more than one as the maximum, one is randomly selected.\n",
    "        max_ind = list(np.flatnonzero(scores == np.max(scores)))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        # Creates a list of the three highest scored indices.\n",
    "        t3_max_ind = np.argsort(np.array(scores))[-3:][::-1]\n",
    "\n",
    "        # The value for chosen is a float, so we avoid using an equality check.\n",
    "        truth_ind = list(np.flatnonzero(batch['won'] == np.max(batch['won'])))[0]\n",
    "        if max_ind == truth_ind:\n",
    "            t1_accuracy += 1\n",
    "            classified['prompts'].append(batch['black_card_text'][max_ind])\n",
    "            classified['punchlines'].append(batch['white_card_text'][max_ind])\n",
    "        else:\n",
    "            misclassified['prompts'].append(batch['black_card_text'][truth_ind])\n",
    "            misclassified['punchlines'].append(batch['white_card_text'][truth_ind])\n",
    "\n",
    "            incorrect['punchlines'].append(batch['white_card_text'][max_ind])\n",
    "\n",
    "        if truth_ind in t3_max_ind[:2]:\n",
    "            t2_accuracy += 1\n",
    "        if truth_ind in t3_max_ind:\n",
    "            t3_accuracy += 1\n",
    "\n",
    "        if reward:\n",
    "            rewards += toxicity_model(**toxicity_tokenizer(batch['inputs'][max_ind], padding=True, truncation=True, return_tensors=\"pt\")).logits.tolist()[0][0] * -1\n",
    "\n",
    "            truth_rewards += toxicity_model(**toxicity_tokenizer(batch['inputs'][truth_ind], padding=True, truncation=True, return_tensors=\"pt\")).logits.tolist()[0][0] * -1\n",
    "\n",
    "        # Resets the pipeline's call count to avoid any warnings.\n",
    "        if model_name != \"popularity\":\n",
    "            generator.call_count = 0\n",
    "\n",
    "    # Clears the memory/cache used from the generation process.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Stores the evaluation metrics and returns them.\n",
    "    t1_accuracy /= batch_len\n",
    "    t2_accuracy /= batch_len\n",
    "    t3_accuracy /= batch_len\n",
    "    rewards /= batch_len\n",
    "    truth_rewards /= batch_len\n",
    "\n",
    "    classified['prompts'] = sort_results(dict(zip(*np.unique(classified['prompts'], return_counts=True))))\n",
    "    classified['punchlines'] = sort_results(dict(zip(*np.unique(classified['punchlines'], return_counts=True))))\n",
    "\n",
    "    misclassified['prompts'] = sort_results(dict(zip(*np.unique(misclassified['prompts'], return_counts=True))))\n",
    "    misclassified['punchlines'] = sort_results(dict(zip(*np.unique(misclassified['punchlines'], return_counts=True))))\n",
    "\n",
    "    incorrect['punchlines'] = sort_results(dict(zip(*np.unique(incorrect['punchlines'], return_counts=True))))\n",
    "\n",
    "    results = {\"t1_accuracy\": t1_accuracy, \"t2_accuracy\": t2_accuracy, \"t3_accuracy\": t3_accuracy, \"classified\": classified, \"misclassified\": misclassified, \"incorrect\": incorrect, \"rewards\": rewards, \"truth_rewards\": truth_rewards}\n",
    "    return results\n",
    "\n",
    "# Sorts the input dictionaries counts in descending order.\n",
    "def sort_results(dictionary):\n",
    "    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the results.\n",
    "def display_results(results, reward=False):\n",
    "    print(f\"Top-1 accuracy: {results['t1_accuracy'] * 100}%\")\n",
    "    print(f\"Top-2 accuracy: {results['t2_accuracy'] * 100}%\")\n",
    "    print(f\"Top-3 accuracy: {results['t3_accuracy'] * 100}%\")\n",
    "\n",
    "    if reward:\n",
    "        print(f\"Average reward: {results['rewards']}\")\n",
    "        print(f\"Truth average reward: {results['truth_rewards']}\")\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nCorrectly predicted prompts:\")\n",
    "    for key, value in results[\"classified\"][\"prompts\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nCorrectly predicted punchlines:\")\n",
    "    for key, value in results[\"classified\"][\"punchlines\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nUnpredicted winning prompts:\")\n",
    "    for key, value in results[\"misclassified\"][\"prompts\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nUnpredicted winning punchlines:\")\n",
    "    for key, value in results[\"misclassified\"][\"punchlines\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nIncorrectly predicted punchlines:\")\n",
    "    for key, value in results[\"incorrect\"][\"punchlines\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "# Displays a graph comparing the results from two models.\n",
    "def display_graph(list_1, list_2, title, metric, reward=None):\n",
    "    x_points1, y_values1 = zip(*list_1)\n",
    "    x_points2, y_values2 = zip(*list_2)\n",
    "\n",
    "    plt.plot(x_points1, y_values1, label='Model 1', marker='o')\n",
    "    plt.plot(x_points2, y_values2, label='Model 2', marker='x')\n",
    "\n",
    "    if reward is not None:\n",
    "        x_points3, y_values3 = zip(*reward)\n",
    "        plt.plot(x_points3, y_values3, label=\"Reward\", marker='*')\n",
    "\n",
    "    plt.xlabel(metric.capitalize() + \"s\")\n",
    "    plt.ylabel(metric.capitalize() + '% Increase')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tick_params(axis='x', labelrotation=80)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def save_results(results, name=\"cah_results\"):\n",
    "    name_list = [result['model_name'] for result in results]\n",
    "    top1_list = [result['t1_accuracy'] for result in results]\n",
    "    top2_list = [result['t2_accuracy'] for result in results]\n",
    "    top3_list = [result['t3_accuracy'] for result in results]\n",
    "    reward_list = [result['rewards'] for result in results]\n",
    "\n",
    "    with open(f\"{name}.csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        header = ['name', 't1_accuracy', 't2_accuracy', 't3_accuracy', 'rewards']\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for i in range(len(name_list)):\n",
    "            row = [name_list[i], top1_list[i], top2_list[i], top3_list[i], reward_list[i]]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "# Displays a graph for the results of an evaluated set of models.\n",
    "def display_models(results, results_2=None, legend=None, iterations=25, reward=False, save=False):\n",
    "    plt.style.use('seaborn') # I personally prefer seaborn for the graph style, but you may choose whichever you want.\n",
    "    params = {\"ytick.color\" : \"black\",\n",
    "              \"xtick.color\" : \"black\",\n",
    "              \"axes.labelcolor\" : \"black\",\n",
    "              \"axes.edgecolor\" : \"black\",\n",
    "              \"text.usetex\" : True,\n",
    "              \"font.family\" : \"serif\",\n",
    "              \"font.serif\" : [\"Computer Modern Serif\"],\n",
    "              \"font.weight\" : \"bold\",\n",
    "              \"font.size\" : 15,\n",
    "              \"legend.fontsize\" : 12,\n",
    "              \"axes.labelsize\" : 14,\n",
    "              \"axes.titlesize\" : 16,\n",
    "              \"xtick.labelsize\" : 13,\n",
    "              \"ytick.labelsize\" : 13}\n",
    "    plt.rcParams.update(params)\n",
    "\n",
    "    top1_list = [result['t1_accuracy'] for result in results]\n",
    "\n",
    "    top2_list = [result['t2_accuracy'] for result in results]\n",
    "    top3_list = [result['t3_accuracy'] for result in results]\n",
    "    reward_list = [result['rewards'] for result in results]\n",
    "\n",
    "    if results_2:\n",
    "        top1_list_2 = [result['t1_accuracy'] for result in results_2]\n",
    "        reward_list_2 = [result['rewards'] for result in results_2]\n",
    "\n",
    "    # Plots the Top-1 accuracy.\n",
    "    if results_2:\n",
    "        plt.plot([num * iterations for num in range(0, len(results))], top1_list, color='#70000c', label=legend[0])\n",
    "        plt.plot([num * iterations for num in range(0, len(results))], top1_list_2, color='#376c67', label=legend[1])\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot([num * iterations for num in range(0, len(results))], top1_list, color='#70000c')\n",
    "    plt.xlabel(r'\\textbf{Iterations}')\n",
    "    plt.ylabel(r'\\textbf{Accuracy}')\n",
    "    plt.title(r'\\textbf{PPO iterations trained against accuracy}')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\"t1-acc.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plots the Top-2 accuracy.\n",
    "    plt.plot([num * iterations for num in range(0, len(results))], top2_list, color='#70000c')\n",
    "    plt.xlabel(r'\\textbf{Iterations}')\n",
    "    plt.ylabel(r'\\textbf{Top-2 Accuracy}')\n",
    "    plt.title(r'\\textbf{PPO iterations trained against Top-2 Accuracy}')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\"t2-acc.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plots the Top-3 accuracy.\n",
    "    plt.plot([num * iterations for num in range(0, len(results))], top3_list, color='#70000c')\n",
    "    plt.xlabel(r'\\textbf{Iterations}')\n",
    "    plt.ylabel(r'\\textbf{Top-3 Accuracy}')\n",
    "    plt.title(r'\\textbf{PPO iterations trained against Top-3 Accuracy}')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(\"t3-acc.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plots the average rewards.\n",
    "    if reward:\n",
    "        if results_2:\n",
    "            plt.plot([num * iterations for num in range(0,len(results))], reward_list, color='#70000c', label=legend[0])\n",
    "            plt.plot([num * iterations for num in range(0, len(results))], reward_list_2, color='#376c67', label=legend[1])\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.plot([num * iterations for num in range(0,len(results))], reward_list, color='#70000c')\n",
    "        plt.xlabel(r'\\textbf{Iterations}')\n",
    "        plt.ylabel(r'\\textbf{Average Reward}')\n",
    "        plt.title(r'\\textbf{PPO iterations trained against average reward}')\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(\"reward.png\", bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Loads results from previous evaluations, saved in a .csv, and displays them on graphs.\n",
    "def load_results(dir, dir_2=None):\n",
    "    results = []\n",
    "    with open(f'{dir}/cah_results.csv', 'r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            for key, value in row.items():\n",
    "                try:\n",
    "                    row[key] = float(value)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            results.append(row)\n",
    "\n",
    "    legend = None\n",
    "    results_2 = None\n",
    "    if dir_2:\n",
    "        legend = ['Humicroedit', 'Cards Against Humanity']\n",
    "        results_2 = []\n",
    "        with open(f'{dir_2}/cah_results.csv', 'r') as file:\n",
    "            csv_reader = csv.DictReader(file)\n",
    "\n",
    "            for row in csv_reader:\n",
    "                for key, value in row.items():\n",
    "                    try:\n",
    "                        row[key] = float(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                results_2.append(row)\n",
    "\n",
    "    display_models(results, results_2, legend, reward=True, save=True)\n",
    "\n",
    "load_results(\"Iterations/BERT-final/Toxicity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Individual Models\n",
    "This section is primarily for measuring the performance or style of individual models, by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on some, or all, of the test data.\n",
    "def eval_data(num=0, seed=None, reward=False):\n",
    "    dataset = pd.read_csv('Data/smaller_test_cah_data.csv')\n",
    "\n",
    "    if num > 0:\n",
    "        ids = dataset['round_id'].unique()\n",
    "        ids = pd.Series(ids).sample(n=num, random_state=seed)\n",
    "        dataset = dataset[dataset['round_id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", str(row['clean_white_card_text']))\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + str(row['white_card_text']), axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "\n",
    "    results = eval_model(dataset, 'popularity', reward)\n",
    "    display_results(results, reward)\n",
    "\n",
    "eval_data(reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on both the trimmed and non-trimmed data.\n",
    "def eval_complete_data(num=0, seed=None, reward=False):\n",
    "    train_dataset = pd.read_csv('Data/train_cah_data.csv')\n",
    "    dataset = pd.read_csv('Data/Old/proc_cah_data.csv')\n",
    "\n",
    "    train_ids = train_dataset['round_id'].unique()\n",
    "    ids = dataset[~dataset[\"round_id\"].isin(train_ids)]['round_id'].unique()\n",
    "    dataset = dataset[dataset['round_id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    if num > 0:\n",
    "        ids = dataset['round_id'].unique()\n",
    "        ids = pd.Series(ids).sample(n=num, random_state=seed)\n",
    "        dataset = dataset[dataset['round_id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['white_card_text'])\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "    results = eval_model(dataset, 'Iterations/BERT-final', reward)\n",
    "    display_results(results, reward)\n",
    "\n",
    "eval_complete_data(num=10000, reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on a split of the test data, based on the domain ratio value for the winning joke.\n",
    "def eval_metrics(num=0, seed=None, reward=False):\n",
    "    dataset = pd.read_csv('Data/test_cah_data.csv')\n",
    "\n",
    "    average_ratio = dataset['domain_ratio'].mean()\n",
    "    dataset = dataset[dataset['domain_ratio'] <= average_ratio]\n",
    "\n",
    "    if num > 0:\n",
    "        ids = dataset['round_id'].unique()\n",
    "        ids = pd.Series(ids).sample(n=num, random_state=seed)\n",
    "        dataset = dataset[dataset['round_id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", str(row['clean_white_card_text']))\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + str(row['white_card_text']), axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "\n",
    "    results = eval_model(dataset, 'Iterations/BERT-final/Positivity/125', reward)\n",
    "    display_results(results, reward)\n",
    "\n",
    "eval_metrics(reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on a split of the data containing only unseen punchlines.\n",
    "# If win is True, we evaluate on rows where the winning joke contains an unseen punchline.\n",
    "def eval_unseen(win=False):\n",
    "    train_dataset = pd.read_csv('Data/train_cah_data.csv')\n",
    "    dataset = pd.read_csv('Data/proc_cah_data.csv')\n",
    "\n",
    "    if win:\n",
    "        unique_punchlines = train_dataset[\"chosen_white_card\"].unique()\n",
    "        u_dataset = dataset[~dataset[\"white_card_text\"].isin(unique_punchlines) & (dataset[\"won\"] == 1)]\n",
    "\n",
    "        round_ids = u_dataset[\"round_id\"].unique()\n",
    "    else:\n",
    "        unique_punchlines = train_dataset[\"white_card_text\"].unique()\n",
    "        u_dataset = dataset[~dataset[\"white_card_text\"].isin(unique_punchlines)]\n",
    "\n",
    "        round_count = u_dataset[\"round_id\"].value_counts()\n",
    "        round_ids = round_count[round_count == 10].index\n",
    "    dataset = dataset[dataset[\"round_id\"].isin(round_ids)]\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['white_card_text'])\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "    results = eval_model(dataset, 'Iterations/BERT-fixed/')\n",
    "    display_results(results)\n",
    "\n",
    "eval_unseen(win=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on a randomly generated rounds.\n",
    "def eval_random(round_num):\n",
    "    # Defines variables necessary for the process.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    generator = pipeline(\"text-classification\", model=\"Iterations/BERT-final/Positivity/250\", batch_size = 10, tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "\n",
    "    # toxicity_tokenizer = RobertaTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "    # toxicity_model = RobertaForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "\n",
    "    # Reads the data and creates a dataframe of every unique prompt and punchline.\n",
    "    dataset = pd.read_csv('Data/proc_cah_data.csv')\n",
    "    prompts = dataset['black_card_text'].unique()\n",
    "    punchlines = dataset['white_card_text'].unique()\n",
    "\n",
    "    # For the specified number of rounds, samples a prompt and ten punchlines for the model to select a winning joke from within.\n",
    "    for n in range(round_num):\n",
    "        prompt = random.choice(prompts)\n",
    "        punchline = [random.choice(punchlines) for num in range(0,10)]\n",
    "        scores = []\n",
    "        jokes = []\n",
    "\n",
    "        # Combines the prompt and punchlines into jokes and generates a score for them.\n",
    "        print(\"Choices: \")\n",
    "        for joke in punchline:\n",
    "            if prompt.count(\"_____\") == 0:\n",
    "                g_joke = prompt + \" \" + joke\n",
    "            else:\n",
    "                g_joke = prompt.replace(\"_____\", joke[0].lower() + joke[1:-1])\n",
    "            jokes.append(g_joke)\n",
    "            print(g_joke)\n",
    "            scores.append(generator(g_joke)[0]['score'])\n",
    "\n",
    "        # Finds the index of the maximum and minimum scores.\n",
    "        max_ind = list(np.flatnonzero(scores == np.max(scores)))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        min_ind = list(np.flatnonzero(scores == np.min(scores)))\n",
    "        min_ind = sample(min_ind,1)[0]\n",
    "\n",
    "        # Prints out the highest and lowest scoring jokes for the round.\n",
    "        print(\"\\nHighest scored joke:\")\n",
    "        print(jokes[max_ind])\n",
    "        print(\"\\nLowest scored joke:\")\n",
    "        print(jokes[min_ind] + \"\\n\")\n",
    "\n",
    "        print(\"Scores:\",scores)\n",
    "\n",
    "        # Resets the pipeline's call count to avoid any warnings.\n",
    "        generator.call_count = 0\n",
    "\n",
    "eval_random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Evaluates a model via SHAP values.\n",
    "def eval_shap(model_name=\"BERT-final\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    generator = pipeline(\"text-classification\", model=f'Iterations/{model_name}', tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "    explainer = shap.Explainer(generator)\n",
    "\n",
    "    dataset = pd.read_csv('Data/Microedit/test_micro_data.csv')\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['white_card_text'])\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "    dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "    shap_values = explainer(dataset['inputs'][:1000], fixed_context=1)\n",
    "\n",
    "    return shap_values\n",
    "\n",
    "shap_values = eval_shap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from bertviz import head_view\n",
    "\n",
    "# Evaluates a model via BertViz.\n",
    "def eval_bertviz():\n",
    "    dataset = pd.read_csv('Data/test_cah_data.csv')\n",
    "    dataset = dataset[dataset['won'] == 1]\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['clean_white_card_text'])\n",
    "                                           if \"_____\" in row['black_card_text']\n",
    "                                           else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('Iterations/BERT-final', local_files_only=True, output_attentions=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    num = random.randint(0, len(dataset['inputs']))\n",
    "    inputs = tokenizer.encode(dataset['inputs'][num], return_tensors='pt')\n",
    "    outputs = model(inputs)\n",
    "    attention = outputs[-1]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "\n",
    "    head_view(attention, tokens)\n",
    "\n",
    "eval_bertviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import together\n",
    "\n",
    "together.api_key = 'KEY'\n",
    "together.Models.start('togethercomputer/llama-2-70b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Evaluates the performance of an LLM (Llama 70B) on the data.\n",
    "def eval_LLM(cont=False):\n",
    "    fsc_prompt = \"SELECT THE FUNNIEST JOKE:\\n(jokes removed as offensive)\\nSELECTION:\"\n",
    "\n",
    "    dataset = pd.read_csv('Data/test_cah_data.csv')\n",
    "    save_dataset = pd.read_csv('Data/test_cah_data.csv')\n",
    "\n",
    "    if not cont:\n",
    "        save_dataset['llm_pred'] = 0\n",
    "    save_dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['clean_white_card_text'])\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['clean_white_card_text'])\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "\n",
    "    for batch in tqdm(dataset, desc=\"Evaluation progress\"):\n",
    "        round_id = batch['round_id'][0]\n",
    "        if (save_dataset[save_dataset['round_id'] == round_id][\"llm_pred\"] == 1).any():\n",
    "            continue\n",
    "\n",
    "        jokes = fsc_prompt + \"\\n\\nSELECT THE FUNNIEST JOKE:\"\n",
    "        truth_ind = list(np.flatnonzero(batch['won'] == np.max(batch['won'])))[0]\n",
    "\n",
    "        for joke in batch['inputs']:\n",
    "            jokes = f\"{jokes}\\n{joke}\"\n",
    "\n",
    "        jokes = jokes + \"\\nSELECTION:  \"\n",
    "        # print(jokes)\n",
    "        # print(\"Winner: \", batch['inputs'][truth_ind])\n",
    "\n",
    "        output = together.Complete.create(\n",
    "          prompt = jokes,\n",
    "          model = \"togethercomputer/llama-2-70b\",\n",
    "          max_tokens = 512,\n",
    "          temperature = 0.7,\n",
    "          top_k = 50,\n",
    "          top_p = 0.7,\n",
    "          repetition_penalty = 1,\n",
    "          stop = ['SELECT THE FUNNIEST JOKE:']\n",
    "        )\n",
    "\n",
    "        selection = (output['prompt'][0]+output['output']['choices'][0]['text'])\n",
    "        selection = selection.split(\"SELECTION:  \")[-1].replace(\"SELECT THE FUNNIEST JOKE:\", \"\").strip()\n",
    "        # print(\"Response:\", selection)\n",
    "\n",
    "        save_dataset.loc[(save_dataset['round_id'] == round_id) & (save_dataset['inputs'] == selection), 'llm_pred'] = 1\n",
    "\n",
    "        save_dataset.to_csv(f\"Data/test_cah_data.csv\", index=False)\n",
    "\n",
    "    save_dataset.drop(columns=['inputs'], inplace=True)\n",
    "    save_dataset.to_csv(f\"Data/test_cah_data.csv\", index=False)\n",
    "\n",
    "eval_LLM(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the performance of models on the Reddit data.\n",
    "def eval_reddit(seed=None, reward=False):\n",
    "    dataset = pd.read_csv('Data/Experimental Data/train_reddit_data.csv')\n",
    "    dataset = dataset.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "    dataset['inputs'] = dataset['joke']\n",
    "    dataset['black_card_text'] = dataset['body']\n",
    "    dataset['white_card_text'] = dataset['punchline']\n",
    "\n",
    "    num_rounds = len(dataset) // 10\n",
    "    round_ids = np.repeat(range(1, num_rounds + 1), 10)\n",
    "    round_ids = np.append(round_ids, np.arange(num_rounds * 10, len(dataset)) + 1)\n",
    "    dataset['round_id'] = round_ids\n",
    "\n",
    "    round_counts = dataset['round_id'].value_counts()\n",
    "    valid_rounds = round_counts[round_counts == 10].index\n",
    "    dataset = dataset[dataset['round_id'].isin(valid_rounds)]\n",
    "\n",
    "    dataset = dataset.groupby('round_id').apply(lambda x: x.sort_values('normalized_score', ascending=False)).reset_index(drop=True)\n",
    "    dataset['won'] = 0.0\n",
    "    dataset.loc[dataset.groupby('round_id').head(1).index, 'won'] = 1.0\n",
    "    dataset.to_csv('Data/train_reddit_data.csv', index=False)\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "\n",
    "    results = eval_model(dataset, 'Iterations/BERT-reddit', reward)\n",
    "    display_results(results, reward)\n",
    "\n",
    "eval_reddit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiple Models\n",
    "This section is centred around comparing separate models and their performance or styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on all of the test data.\n",
    "def eval_data(models=['Iterations/BERT-final'], iterations=0, reward=False, name=\"cah_results\"):\n",
    "    dataset = pd.read_csv('Data/test_cah_data.csv')\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", str(row['clean_white_card_text']))\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + str(row['white_card_text']), axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "\n",
    "    # Iterates through every specified model, displaying and storing their results as they are evaluated.\n",
    "    total_results = []\n",
    "    for model in models:\n",
    "        results = eval_model(dataset, model, reward)\n",
    "        display_results(results, reward)\n",
    "        results['model_name'] = model\n",
    "\n",
    "        total_results.append(results)\n",
    "\n",
    "    # Displays the total results on a graph if we are evaluating iterations of a model.\n",
    "    if iterations > 0:\n",
    "        save_results(total_results, name)\n",
    "        display_models(total_results, iterations, reward)\n",
    "\n",
    "itr = 0\n",
    "models = ['Iterations/BERT-final']\n",
    "for num in range(0, 20):\n",
    "    itr += 25\n",
    "    models.append(f'Iterations/BERT-final/RLHF{itr}')\n",
    "\n",
    "eval_data(models, 25, reward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on a randomly generated rounds.\n",
    "def eval_random(round_num):\n",
    "    # Defines variables necessary for the process.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    generator = pipeline(\"text-classification\", model=\"Iterations/BERT-fixed\", batch_size = 10, tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "\n",
    "    generator_2 = pipeline(\"text-classification\", model=\"Iterations/BERT-fixed/RawStyle/Toxicity2/25\", batch_size = 10, tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "\n",
    "    generator_3 = pipeline(\"text-classification\", model=\"Iterations/BERT-fixed/RawStyle/Toxicity2/75\", batch_size = 10, tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "\n",
    "    toxicity_tokenizer = RobertaTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "    toxicity_model = RobertaForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "\n",
    "    # Reads the data and creates a dataframe of every unique prompt and punchline.\n",
    "    dataset = pd.read_csv('Data/proc_cah_data.csv')\n",
    "    prompts = dataset['black_card_text'].unique()\n",
    "    punchlines = dataset['white_card_text'].unique()\n",
    "\n",
    "    # For the specified number of rounds, samples a prompt and ten punchlines for the model to select a winning joke from within.\n",
    "    for n in range(round_num):\n",
    "        prompt = random.choice(prompts)\n",
    "        punchline = [random.choice(punchlines) for num in range(0,10)]\n",
    "        scores = []\n",
    "        scores_2 = []\n",
    "        scores_3 = []\n",
    "        jokes = []\n",
    "\n",
    "        # Combines the prompt and punchlines into jokes and generates a score for them.\n",
    "        print(\"Choices: \")\n",
    "        for joke in punchline:\n",
    "            if prompt.count(\"_____\") == 0:\n",
    "                g_joke = prompt + \" \" + joke\n",
    "            else:\n",
    "                g_joke = prompt.replace(\"_____\", joke[0].lower() + joke[1:-1])\n",
    "            jokes.append(g_joke)\n",
    "\n",
    "            scores.append(generator(g_joke)[0]['score'])\n",
    "            scores_2.append(generator_2(g_joke)[0]['score'])\n",
    "            scores_3.append(generator_3(g_joke)[0]['score'])\n",
    "\n",
    "            # Resets the pipeline's call count to avoid any warnings.\n",
    "            generator.call_count = 0\n",
    "\n",
    "\n",
    "        # Finds the index of the maximum and minimum scores.\n",
    "        max_ind = list(np.flatnonzero(scores == np.max(scores)))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        min_ind = list(np.flatnonzero(scores == np.min(scores)))\n",
    "        min_ind = sample(min_ind,1)[0]\n",
    "\n",
    "        # Prints out the highest and lowest scoring jokes for the round.\n",
    "        print(\"[1] Highest scored joke:\")\n",
    "        print(f\"{jokes[max_ind]} [{(toxicity_model(**toxicity_tokenizer(jokes[max_ind], padding=True, truncation=True, return_tensors='pt')).logits.tolist()[0][0] * -1)}]\")\n",
    "        print(\"\\n[1] Lowest scored joke:\")\n",
    "        print(f\"{jokes[min_ind]} [{(toxicity_model(**toxicity_tokenizer(jokes[min_ind], padding=True, truncation=True, return_tensors='pt')).logits.tolist()[0][0] * -1)}]\\n\")\n",
    "\n",
    "        # Finds the index of the maximum and minimum scores.\n",
    "        max_ind = list(np.flatnonzero(scores_2 == np.max(scores_2)))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        min_ind = list(np.flatnonzero(scores_2 == np.min(scores_2)))\n",
    "        min_ind = sample(min_ind,1)[0]\n",
    "\n",
    "        # Prints out the highest and lowest scoring jokes for the round.\n",
    "        print(\"[2] Highest scored joke:\")\n",
    "        print(f\"{jokes[max_ind]} [{(toxicity_model(**toxicity_tokenizer(jokes[max_ind], padding=True, truncation=True, return_tensors='pt')).logits.tolist()[0][0] * -1)}]\")\n",
    "        print(\"\\n[2] Lowest scored joke:\")\n",
    "        print(f\"{jokes[min_ind]} [{(toxicity_model(**toxicity_tokenizer(jokes[min_ind], padding=True, truncation=True, return_tensors='pt')).logits.tolist()[0][0] * -1)}]\\n\")\n",
    "\n",
    "        # Finds the index of the maximum and minimum scores.\n",
    "        max_ind = list(np.flatnonzero(scores_3 == np.max(scores_3)))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        min_ind = list(np.flatnonzero(scores_3 == np.min(scores_3)))\n",
    "        min_ind = sample(min_ind,1)[0]\n",
    "\n",
    "        # Prints out the highest and lowest scoring jokes for the round.\n",
    "        print(\"[3] Highest scored joke:\")\n",
    "        print(f\"{jokes[max_ind]} [{(toxicity_model(**toxicity_tokenizer(jokes[max_ind], padding=True, truncation=True, return_tensors='pt')).logits.tolist()[0][0] * -1)}]\")\n",
    "        print(\"\\n[3] Lowest scored joke:\")\n",
    "        print(f\"{jokes[min_ind]} [{(toxicity_model(**toxicity_tokenizer(jokes[min_ind], padding=True, truncation=True, return_tensors='pt')).logits.tolist()[0][0] * -1)}]\\n\")\n",
    "\n",
    "eval_random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes non-alphanumeric characters from input words.\n",
    "def remove_characters(word):\n",
    "    alphanumeric_matches = re.findall(r'[a-zA-Z0-9]+', word)\n",
    "    main_string = ''.join(alphanumeric_matches)\n",
    "    return main_string\n",
    "\n",
    "# Remove duplicate elements in word lists.\n",
    "def remove_duplicates(word_list):\n",
    "    seen_words = {}\n",
    "    final_list = []\n",
    "\n",
    "    for word in word_list:\n",
    "        curr_word = word[0]\n",
    "\n",
    "        if curr_word not in seen_words:\n",
    "            seen_words[curr_word] = True\n",
    "            final_list.append(word)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "# Measures the impacts certain words have on the overall rating of a joke.\n",
    "def eval_style(num, style=\"words\", reward=False, scale=5, seed=None):\n",
    "    # Defines variables necessary for the process.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    generator = pipeline(\"text-classification\", model=\"Iterations/BERT-fixed/RawStyle/Toxicity2/25\", batch_size = 1, tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "    generator_2 = pipeline(\"text-classification\", model=\"Iterations/BERT-fixed/RawStyle/Toxicity2/75\", batch_size = 1, tokenizer=tokenizer, device=0, function_to_apply=\"none\")\n",
    "\n",
    "    # If reward is required, we also define the reward models.\n",
    "    if reward:\n",
    "        toxicity_tokenizer = RobertaTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "        toxicity_model = RobertaForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "\n",
    "    # Reads the data and creates a dataframe of every unique prompt and punchline.\n",
    "    dataset = pd.read_csv('Data/proc_cah_data.csv')\n",
    "\n",
    "    ids = dataset['round_id'].unique()\n",
    "    ids = pd.Series(ids).sample(n=num, random_state=seed)\n",
    "    dataset = dataset[dataset['round_id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['white_card_text'])\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "\n",
    "    word_list = {}\n",
    "    word_list_2 = {}\n",
    "    reward_list = {}\n",
    "    word_count = {}\n",
    "    for batch in tqdm(dataset, desc=\"Evaluation progress\"):\n",
    "        # If we track punchlines, get the base score from the score of the joke prompt.\n",
    "        if style == \"punchline\":\n",
    "            base_score = generator(batch['black_card_text'][0])[0]['score'] + scale\n",
    "            base_score_2 = generator_2(batch['black_card_text'][0])[0]['score'] + scale\n",
    "\n",
    "            if reward:\n",
    "                base_reward = toxicity_model(**toxicity_tokenizer(batch['black_card_text'][0], padding=True, truncation=True, return_tensors=\"pt\")).logits.tolist()\n",
    "                base_reward = ((base_reward[0][0] * -1) + scale)\n",
    "\n",
    "        for index, joke in enumerate(batch['inputs']):\n",
    "            # If the desired metric is per word then iterate through every word in the jokes.\n",
    "            if style == \"word\":\n",
    "                prev_score = 0\n",
    "                prev_score_2 = 0\n",
    "                curr_word = \"\"\n",
    "\n",
    "                for word in joke.split():\n",
    "                    curr_word += word\n",
    "                    word = remove_characters(word)\n",
    "\n",
    "                    new_score = generator(curr_word)[0]['score'] + scale\n",
    "                    new_score_2 = generator_2(curr_word)[0]['score'] + scale\n",
    "                    generator.call_count = 0\n",
    "                    generator_2.call_count = 0\n",
    "\n",
    "                    if prev_score == 0:\n",
    "                        prev_score = new_score\n",
    "                        prev_score_2 = new_score_2\n",
    "                        continue\n",
    "\n",
    "                    if word not in word_list:\n",
    "                        word_list[word] = 0\n",
    "                        word_list_2[word] = 0\n",
    "                        word_count[word] = 0\n",
    "\n",
    "                    word_list[word] += new_score/prev_score\n",
    "                    word_list_2[word] += new_score_2/prev_score_2\n",
    "                    word_count[word] += 1\n",
    "\n",
    "                    prev_score = new_score\n",
    "                    prev_score_2 = new_score_2\n",
    "\n",
    "            # If the desired metric is per punchline then iterate through every joke.\n",
    "            if style == \"punchline\":\n",
    "                punchline = batch['white_card_text'][index]\n",
    "                score = generator(joke)[0]['score'] + scale\n",
    "                score_2 = generator_2(joke)[0]['score'] + scale\n",
    "                generator.call_count = 0\n",
    "                generator_2.call_count = 0\n",
    "\n",
    "                if joke not in word_list:\n",
    "                    word_list[punchline] = 0\n",
    "                    word_list_2[punchline] = 0\n",
    "                    word_count[punchline] = 0\n",
    "\n",
    "                    if reward:\n",
    "                        reward_list[punchline] = 0\n",
    "\n",
    "                word_list[punchline] += score/base_score\n",
    "                word_list_2[punchline] += score_2/base_score_2\n",
    "                word_count[punchline] += 1\n",
    "\n",
    "                if reward:\n",
    "                    curr_reward = toxicity_model(**toxicity_tokenizer(joke, padding=True, truncation=True, return_tensors=\"pt\")).logits.tolist()\n",
    "                    curr_reward = ((curr_reward[0][0] * -1) + scale)\n",
    "\n",
    "                    reward_list[punchline] += (curr_reward/base_reward)/(scale * 2) + 0.5\n",
    "\n",
    "\n",
    "\n",
    "    # Averages every score shift value by the number of times a word appears.\n",
    "    for word in word_list:\n",
    "        word_list[word] /= word_count[word]\n",
    "        word_list_2[word] /= word_count[word]\n",
    "\n",
    "        if reward:\n",
    "            reward_list[word] /= word_count[word]\n",
    "\n",
    "    # Grabs the 5 highest and lowest score changing words for each model.\n",
    "    d_word_list = sorted(word_list.items(), key=lambda item: item[1], reverse=False)[0:5]\n",
    "    a_word_list = sorted(word_list.items(), key=lambda item: item[1], reverse=True)[0:5]\n",
    "\n",
    "    d_word_list_2 = sorted(word_list_2.items(), key=lambda item: item[1], reverse=False)[0:5]\n",
    "    a_word_list_2 = sorted(word_list_2.items(), key=lambda item: item[1], reverse=True)[0:5]\n",
    "\n",
    "    for word in range(0,5):\n",
    "        d_word_list.append((d_word_list_2[word][0], word_list[d_word_list_2[word][0]]))\n",
    "        d_word_list_2.append((d_word_list[word][0], word_list_2[d_word_list[word][0]]))\n",
    "\n",
    "        a_word_list.append((a_word_list_2[word][0], word_list[a_word_list_2[word][0]]))\n",
    "        a_word_list_2.append((a_word_list[word][0], word_list_2[a_word_list[word][0]]))\n",
    "\n",
    "    d_word_list_2 = d_word_list_2[5:] + d_word_list_2[0:5]\n",
    "    a_word_list_2 = a_word_list_2[5:] + a_word_list_2[0:5]\n",
    "\n",
    "    d_reward_list, a_reward_list = None, None\n",
    "    if reward:\n",
    "        d_reward_list, a_reward_list = [], []\n",
    "        for word in range(0, 10):\n",
    "            d_reward_list.append((d_word_list[word][0], reward_list[d_word_list[word][0]]))\n",
    "            a_reward_list.append((a_word_list[word][0], reward_list[a_word_list[word][0]]))\n",
    "        d_reward_list = remove_duplicates(d_reward_list)\n",
    "        a_reward_list = remove_duplicates(a_reward_list)\n",
    "\n",
    "    display_graph(remove_duplicates(a_word_list), remove_duplicates(a_word_list_2), \"Top % increasing \"  + style + \"s\", style, a_reward_list)\n",
    "    display_graph(remove_duplicates(d_word_list), remove_duplicates(d_word_list_2), \"Top % decreasing \" + style + \"s\", style, d_reward_list)\n",
    "\n",
    "eval_style(100, \"punchline\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
