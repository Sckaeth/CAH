{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "The code within this notebook is primarily for evaluating the model's performance in an isolated setting.\n",
    "\n",
    "Thus, it will support:\n",
    "1. Testing the model's performance against the test dataset.\n",
    "2. Testing the performance of other models/methodologies on the test dataset.\n",
    "3. Evaluating the model on random samples of the test dataset.\n",
    "4. Testing the performance of the model on individual, randomly generated, game rounds.\n",
    "\n",
    "The following metrics will be printed for each evaluation:\n",
    "- The success rate of predictions (?).\n",
    "- The most misclassified prompts & punchlines.\n",
    "- The most accurately classified prompts & punchlines.\n",
    "(for now these track correctly predicted jokes and jokes that were not predicted at all)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Imports libraries relevant to the evaluation process.\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch, gc, random, tabulate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Evaluates the trained model off an input dataset.\n",
    "def eval_model(dataset):\n",
    "    # Defines necessary evaluation variables.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    generator = pipeline(\"text-classification\", model=\"Iterations/BERT/AT-3\", batch_size = 10, tokenizer=tokenizer, device=0)\n",
    "    batch_len = len(dataset)\n",
    "\n",
    "    # Defines the evaluation metrics.\n",
    "    accuracy = 0\n",
    "    classified = {\"prompts\": [], \"punchlines\":[]}\n",
    "    misclassified = {\"prompts\": [], \"punchlines\": []}\n",
    "\n",
    "    # Iterates through each batch in the data, with each batch representing a round played.\n",
    "    for batch in tqdm(dataset, desc=\"Evaluation progress\"):\n",
    "        # Generates a score for each joke in the batch.\n",
    "        scores = []\n",
    "        for out in generator(KeyDataset(batch, \"inputs\")):\n",
    "            scores.append(out['score'])\n",
    "\n",
    "        # Selects the index of the maximum score. If there is more than one as the maximum, one is randomly selected.\n",
    "        max_ind = list(np.flatnonzero(scores == np.max(scores)))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        # The value for chosen is a float, so we avoid using an equality check.\n",
    "        truth_ind = list(np.flatnonzero(batch['won'] == np.max(batch['won'])))[0]\n",
    "        if batch['won'][max_ind] > 0:\n",
    "            accuracy += 1\n",
    "            classified['prompts'].append(batch['black_card_text'][max_ind])\n",
    "            classified['punchlines'].append(batch['white_card_text'][max_ind])\n",
    "        else:\n",
    "            misclassified['prompts'].append(batch['black_card_text'][truth_ind])\n",
    "            misclassified['punchlines'].append(batch['white_card_text'][truth_ind])\n",
    "\n",
    "        # Resets the pipeline's call count to avoid any warnings.\n",
    "        generator.call_count = 0\n",
    "\n",
    "    # Clears the memory/cache used from the generation process.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Stores the evaluation metrics and returns them.\n",
    "    accuracy /= batch_len\n",
    "\n",
    "    classified['prompts'] = sort_results(dict(zip(*np.unique(classified['prompts'], return_counts=True))))\n",
    "    classified['punchlines'] = sort_results(dict(zip(*np.unique(classified['punchlines'], return_counts=True))))\n",
    "\n",
    "    misclassified['prompts'] = sort_results(dict(zip(*np.unique(misclassified['prompts'], return_counts=True))))\n",
    "    misclassified['punchlines'] = sort_results(dict(zip(*np.unique(misclassified['punchlines'], return_counts=True))))\n",
    "\n",
    "    results = {\"accuracy\": accuracy, \"classified\": classified, \"misclassified\": misclassified}\n",
    "    return results\n",
    "\n",
    "# Sorts the input dictionaries counts in descending order.\n",
    "def sort_results(dictionary):\n",
    "    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Displays the results.\n",
    "def display_results(results):\n",
    "    print(f\"Accuracy: {results['accuracy'] * 100}%\")\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nCorrectly predicted prompts:\")\n",
    "    for key, value in results[\"classified\"][\"prompts\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nCorrectly predicted punchlines:\")\n",
    "    for key, value in results[\"classified\"][\"punchlines\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nIncorrectly predicted prompts:\")\n",
    "    for key, value in results[\"misclassified\"][\"prompts\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    n = 0\n",
    "    print(\"\\nIncorrectly predicted punchlines:\")\n",
    "    for key, value in results[\"misclassified\"][\"punchlines\"].items():\n",
    "        print(f\"{key} - {value}\")\n",
    "        if n < 5:\n",
    "            n += 1\n",
    "            continue\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on all of the test data.\n",
    "def eval_data():\n",
    "    dataset = pd.read_csv('Data/proc_cah_data.csv')\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", str(row['white_card_text']))\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + str(row['white_card_text']), axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "    results = eval_model(dataset)\n",
    "    display_results(results)\n",
    "\n",
    "eval_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on a specified number of games sampled from the test data/\n",
    "def eval_samples(num, seed=None):\n",
    "    dataset = pd.read_csv('Data/proc_cah_data.csv')\n",
    "\n",
    "    ids = dataset['round_id'].unique()\n",
    "    ids = pd.Series(ids).sample(n=num, random_state=seed)\n",
    "    dataset = dataset[dataset['round_id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    dataset['inputs'] = dataset.apply(lambda row: row['black_card_text'].replace(\"_____\", row['white_card_text'])\n",
    "                                       if \"_____\" in row['black_card_text']\n",
    "                                       else row['black_card_text'] + \" \" + row['white_card_text'], axis=1)\n",
    "\n",
    "    dataset = [Dataset.from_pandas(group) for _, group in dataset.groupby('round_id')]\n",
    "    results = eval_model(dataset)\n",
    "    display_results(results)\n",
    "\n",
    "eval_samples(10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performs an evaluation of the model on a randomly generated rounds.ww\n",
    "def eval_random(round_num):\n",
    "    # Defines variables necessary for the process.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    generator = pipeline(\"text-classification\", model=\"Iterations/BERT/AT-3\", batch_size = 10, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # Reads the data and creates a dataframe of every unique prompt and punchline.\n",
    "    dataset = pd.read_csv('Data/proc_cah_data.csv')\n",
    "    prompts = dataset['black_card_text'].unique()\n",
    "    punchlines = dataset['white_card_text'].unique()\n",
    "\n",
    "    # For the specified number of rounds, samples a prompt and ten punchlines for the model to select a winning joke from within.\n",
    "    for n in range(round_num):\n",
    "        prompt = random.choice(prompts)\n",
    "        punchline = [random.choice(punchlines) for num in range(0,10)]\n",
    "        scores = []\n",
    "        jokes = []\n",
    "\n",
    "        # Combines the prompt and punchlines into jokes and generates a score for them.\n",
    "        print(\"Choices: \")\n",
    "        for joke in punchline:\n",
    "            if prompt.count(\"_____\") == 0:\n",
    "                g_joke = prompt + \" \" + joke\n",
    "            else:\n",
    "                g_joke = prompt.replace(\"_____\", joke[0].lower() + joke[1:-1])\n",
    "            jokes.append(g_joke)\n",
    "            print(g_joke)\n",
    "            scores.append(generator(g_joke)[0]['score'])\n",
    "\n",
    "        # Finds the index of the maximum and minimum scores.\n",
    "        max_ind = list(np.flatnonzero(scores == np.max(scores)))\n",
    "        max_ind = sample(max_ind,1)[0]\n",
    "\n",
    "        min_ind = list(np.flatnonzero(scores == np.min(scores)))\n",
    "        min_ind = sample(min_ind,1)[0]\n",
    "\n",
    "        # Prints out the highest and lowest scoring jokes for the round.\n",
    "        print(\"\\nHighest scored joke:\")\n",
    "        print(jokes[max_ind])\n",
    "        print(\"\\nLowest scored joke:\")\n",
    "        print(jokes[min_ind] + \"\\n\")\n",
    "\n",
    "        # Resets the pipeline's call count to avoid any warnings.\n",
    "        generator.call_count = 0\n",
    "\n",
    "eval_random(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Performs an evaluation of other models on the test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
